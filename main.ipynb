{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as platform\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Layer, Dense, Embedding, Dropout,\n",
    "    TextVectorization, LayerNormalization,\n",
    "    MultiHeadAttention, Input\n",
    ")\n",
    "\n",
    "\n",
    "## To replicate the results\n",
    "from tensorflow.random import set_seed\n",
    "from numpy.random import seed\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)\n",
    "seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "## If want to train on CPU instead of GPU\n",
    "tf.config.set_visible_devices([], 'GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_df(df, data):\n",
    "\n",
    "    english_lines = list()\n",
    "    spanish_lines = list()\n",
    "    for data_line in data:\n",
    "        english_line, spanish_line = data_line.split(\"\\t\")\n",
    "\n",
    "        english_lines.append(english_line)\n",
    "        spanish_lines.append(spanish_line)\n",
    "    \n",
    "    df['english'] = english_lines\n",
    "    df['spanish'] = spanish_lines\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_vocab_size(data):\n",
    "\n",
    "    vocab = set()\n",
    "    for data_line in tqdm(data):\n",
    "        vocab = vocab.union(data_line.split())\n",
    "    \n",
    "    return len(vocab)\n",
    "\n",
    "\n",
    "def generate_self_attention_mask(inputs):\n",
    "\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "\n",
    "        self_attention_mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        self_attention_mask = tf.reshape(self_attention_mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        \n",
    "        return tf.tile(self_attention_mask, mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = get_file(\n",
    "    fname=\"dataset.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file_path = f\"{'/'.join(dataset_path.split('/')[:-1])}/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_file_path) as f:\n",
    "    data = f.read().split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['english', 'spanish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = populate_df(df, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english  spanish\n",
       "0     Go.      Ve.\n",
       "1     Go.    Vete.\n",
       "2     Go.    Vaya.\n",
       "3     Go.  Váyase.\n",
       "4     Hi.    Hola."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting to lowercase\n",
    "\n",
    "df['english'] = df['english'].apply(str.lower)\n",
    "df['spanish'] = df['spanish'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replacing puncutation\n",
    "\n",
    "df['english'] = df['english'].apply(lambda line: re.sub(r'[^\\w\\s]', '', line))\n",
    "df['spanish'] = df['spanish'].apply(lambda line: re.sub(r'[^\\w\\s]' + \"¿\", '', line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding <START> and <END> Token\n",
    "\n",
    "df['spanish'] = df['spanish'].apply(\n",
    "    lambda data_line: f'<START> {data_line} <END>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>&lt;START&gt; ve. &lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go</td>\n",
       "      <td>&lt;START&gt; vete. &lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>go</td>\n",
       "      <td>&lt;START&gt; vaya. &lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>go</td>\n",
       "      <td>&lt;START&gt; váyase. &lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi</td>\n",
       "      <td>&lt;START&gt; hola. &lt;END&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english                spanish\n",
       "0      go      <START> ve. <END>\n",
       "1      go    <START> vete. <END>\n",
       "2      go    <START> vaya. <END>\n",
       "3      go  <START> váyase. <END>\n",
       "4      hi    <START> hola. <END>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 118964 training examples in the data\n"
     ]
    }
   ],
   "source": [
    "num_examples = df.shape[0]\n",
    "\n",
    "print(f'There are {num_examples} training examples in the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:\n",
      "\t95171 training examples\n",
      "\t11896 validation examples\n",
      "\t11897 test examples\n"
     ]
    }
   ],
   "source": [
    "train_size = int(num_examples * 0.8)\n",
    "val_size = int(num_examples * 0.1)\n",
    "\n",
    "print(\n",
    "    f'There are:\\n\\t{train_size} training examples\\n\\t{val_size} validation examples\\n\\t{num_examples-(train_size+val_size)} test examples'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:train_size+val_size]\n",
    "test_df = df.iloc[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95171, 2), (11896, 2), (11897, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95171/95171 [00:11<00:00, 8527.20it/s] \n",
      "100%|██████████| 95171/95171 [00:35<00:00, 2667.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 12576 \n",
      "Spanish Vocabulary Size: 37345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "english_vocab_size = get_vocab_size(train_df['english'])\n",
    "spanish_vocab_size = get_vocab_size(train_df['spanish'])\n",
    "\n",
    "print(\n",
    "    f'English Vocabulary Size: {english_vocab_size} \\nSpanish Vocabulary Size: {spanish_vocab_size}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentence_lengths = train_df['english'].map(str.split).map(len)\n",
    "spanish_sentence_lengths = train_df['spanish'].map(str.split).map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    English Sentence Length Stats:\n",
      "    \tMaximum Headline length: 47\n",
      "    \tMinimum Headline length: 1\n",
      "    \tAverage Headline length: 6.31\n",
      "    \tSTD of Headline length: 2.61\n",
      "\n",
      "    Spanish Sentence Length Stats:\n",
      "    \tMaximum Headline length: 51\n",
      "    \tMinimum Headline length: 3\n",
      "    \tAverage Headline length: 8.09\n",
      "    \tSTD of Headline length: 2.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "    English Sentence Length Stats:\n",
    "    \\tMaximum Headline length: {english_sentence_lengths.max()}\n",
    "    \\tMinimum Headline length: {english_sentence_lengths.min()}\n",
    "    \\tAverage Headline length: {english_sentence_lengths.mean():.2f}\n",
    "    \\tSTD of Headline length: {english_sentence_lengths.std():.2f}\n",
    "\n",
    "    Spanish Sentence Length Stats:\n",
    "    \\tMaximum Headline length: {spanish_sentence_lengths.max()}\n",
    "    \\tMinimum Headline length: {spanish_sentence_lengths.min()}\n",
    "    \\tAverage Headline length: {spanish_sentence_lengths.mean():.2f}\n",
    "    \\tSTD of Headline length: {spanish_sentence_lengths.std():.2f}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 17)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sequence_len = math.ceil(english_sentence_lengths.mean() + (3 * english_sentence_lengths.std()))\n",
    "spanish_sequence_len = math.ceil(spanish_sentence_lengths.mean() + (3 * spanish_sentence_lengths.std()))\n",
    "\n",
    "\n",
    "english_sequence_len, spanish_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    max_tokens=english_vocab_size,\n",
    "    output_sequence_length=english_sequence_len,\n",
    ")\n",
    "\n",
    "spanish_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    max_tokens=spanish_vocab_size,\n",
    "    output_sequence_length=spanish_sequence_len + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 15:34:36.056318: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "english_vectorization.adapt(train_df['english'])\n",
    "spanish_vectorization.adapt(train_df['spanish'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets from Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(english_line, spanish_line):\n",
    "    english_line = english_vectorization(english_line)\n",
    "    spanish_line = spanish_vectorization(spanish_line)\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": english_line,\n",
    "            \"decoder_inputs\": spanish_line[:, :-1],\n",
    "        },\n",
    "        spanish_line[:, 1:]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset = Dataset.from_tensor_slices(\n",
    "    ( train_df['english'], train_df['spanish'])).batch(\n",
    "        batch_size).map(\n",
    "            format_dataset).shuffle(\n",
    "                train_df['english'].shape[0]).prefetch(\n",
    "                    batch_size).cache()\n",
    "\n",
    "val_ds = dataset = Dataset.from_tensor_slices(\n",
    "    ( val_df['english'], val_df['spanish'])).batch(\n",
    "        batch_size).map(\n",
    "            format_dataset).shuffle(\n",
    "                val_df['english'].shape[0]).prefetch(\n",
    "                    batch_size).cache()\n",
    "\n",
    "test_ds = dataset = Dataset.from_tensor_slices(\n",
    "    ( test_df['english'], test_df['spanish'])).batch(\n",
    "        batch_size).map(\n",
    "            format_dataset).shuffle(\n",
    "                test_df['english'].shape[0]).prefetch(\n",
    "                    batch_size).cache()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoder_inputs': <tf.Tensor: shape=(32, 15), dtype=int64, numpy=\n",
      "array([[ 108,    5,   75,    4,   19, 6973,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [  78, 5531,   13,  230,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   3,   36, 1195,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [ 522,   29,    2, 1783,   35,  135,  358, 1343,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   3,   32,    5,    4,  219,   17, 3281,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [  97,  919,  232,    4,    6,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [  13,   14,   84, 1327,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [  53, 1065,    2, 2590,  258,   14,  226,   20,  510,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [  34,  285,  102,   95,    4,   41,   90,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [ 134,   33,  253, 1011,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [ 381,  203,    4,  182,   10, 3344,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [  10, 3077,   25,  109,    5,  103,  135, 8170,   20,   25, 1800,\n",
      "          11, 1041,    0,    0],\n",
      "       [   6, 6070,   37,   26,   31, 2486,   35, 5299,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [2387,   55,  148,   90,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   3,   59,   32,    4, 3079,    6,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   9,   95,   17,    7,  288,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [ 128,  147, 1526,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   3,   73,  558, 4907,   89, 1961,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   3,   59,   60,    6,   73,   32,    4,   76,   17,  177,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   3,   18,    7,  242,  667,   10,   19,  113,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [  92,  181,  158,    4,  163,   24,    5,   40,   15,  139,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   6,   43,    2,  624,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   3,   22,   38,  617,    9,   44,   75,  184,   33,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   3,  193,   30,  987,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   6,  446, 1587,   20,  268,  415,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [ 178,   28,  978,  459,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [ 392, 1345,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [  15,   53,    9, 1959,    5,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   6,  434,  615,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   6,  103,  212,   26,   20,  477,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [  30,  569,   31,   28,  686,   11,  671,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   3,   41,    4,  109,  153,  107,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0]])>, 'decoder_inputs': <tf.Tensor: shape=(32, 17), dtype=int64, numpy=\n",
      "array([[    2,   644,   333,     6,    22,   184,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    17, 24213,   203,     3,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    14,   301,    30,  1526,     3,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,  2598,    11,    10,  1335,    29,  2258,  2752,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    45,     5,    14,  1160,    33,   274,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    55,  1424,    26,  1397,     6,     8,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    65,    60,  1548,     3,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    52,     4,  1660,    10,   262,     4,  2634,   384,\n",
      "          115,     6,   269,     3,     0,     0,     0,     0],\n",
      "       [    2,    37,  1330,    88,    14,    47,     5,    67,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    42,     7,    12,   216,  2363,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    54,    10,   194,   107,   298, 22038,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    49,  4260,   145,    20,    84,  1425,    11,   265,\n",
      "          313,    20,   595,     4,  1132,     3,     0,     0],\n",
      "       [    2,     8,   730, 15454,     6,    27,    23,   567,    29,\n",
      "         3877,     3,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    88,    24,    54,    72,     3,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     7,  1111,  9082,     6,     8,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    16,    14,   214,    13,   403,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,  1640,   613,  2549,     3,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,   654,  8892,     4,   537,   123,     5,  1927,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     7,   166,     5,     8,   392,   387,     6,   997,\n",
      "            3,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    47,    13,   390,   507,    11,     9,  1259,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     7,   885,    28,    82,    17,     5,   140,    51,\n",
      "          102,     3,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     8,    40,     9,   474,     3,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     7,    78,    49,   111,     6,   333,   141,     7,\n",
      "            3,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     7,   615,  2764,     3,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     8,    15,  1052,  1858,    19,    10,   356,     4,\n",
      "           27,     3,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,   181,    12,    36,   634,  2992,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,   106,  1512,     3,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,  1652,    17,     5,   213,   870,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     8,   378,   529,     3,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     8,   189,  1555,   576,     6,    27,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2, 10530,   557,    23,    17,     5,  5208,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,   105,     6,   315,    61,    21,   146,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]])>}\n",
      "tf.Tensor(\n",
      "[[  644   333     6    22   184     3     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   17 24213   203     3     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   14   301    30  1526     3     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [ 2598    11    10  1335    29  2258  2752     3     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   45     5    14  1160    33   274     3     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   55  1424    26  1397     6     8     3     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   65    60  1548     3     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   52     4  1660    10   262     4  2634   384   115     6   269     3\n",
      "      0     0     0     0     0]\n",
      " [   37  1330    88    14    47     5    67     3     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   42     7    12   216  2363     3     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   54    10   194   107   298 22038     3     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   49  4260   145    20    84  1425    11   265   313    20   595     4\n",
      "   1132     3     0     0     0]\n",
      " [    8   730 15454     6    27    23   567    29  3877     3     0     0\n",
      "      0     0     0     0     0]\n",
      " [   88    24    54    72     3     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    7  1111  9082     6     8     3     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   16    14   214    13   403     3     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [ 1640   613  2549     3     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [  654  8892     4   537   123     5  1927     3     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    7   166     5     8   392   387     6   997     3     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   47    13   390   507    11     9  1259     3     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    7   885    28    82    17     5   140    51   102     3     0     0\n",
      "      0     0     0     0     0]\n",
      " [    8    40     9   474     3     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    7    78    49   111     6   333   141     7     3     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    7   615  2764     3     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    8    15  1052  1858    19    10   356     4    27     3     0     0\n",
      "      0     0     0     0     0]\n",
      " [  181    12    36   634  2992     3     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [  106  1512     3     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [ 1652    17     5   213   870     3     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    8   378   529     3     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    8   189  1555   576     6    27     3     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [10530   557    23    17     5  5208     3     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [  105     6   315    61    21   146     3     0     0     0     0     0\n",
      "      0     0     0     0     0]], shape=(32, 17), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 15:35:07.337678: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_ds.take(1):\n",
    "    print(X)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Architecture of Blocks of Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim,  sequence_length, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.token_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.position_embeddings = Embedding(input_dim=sequence_length, output_dim=embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        positions = tf.range(start=0, limit=tf.shape(inputs)[-1], delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        out = embedded_tokens + embedded_positions\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(Layer):\n",
    "\n",
    "    def __init__(self, dense_dim, out_dim, dropout_p, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.dense = Sequential([\n",
    "            Dense(dense_dim, activation='relu'), \n",
    "            Dropout(dropout_p),\n",
    "            Dense(out_dim, activation='relu')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(Layer):\n",
    "\n",
    "    def __init__(self, embedding_dim, dense_dim, num_heads, dropout_p, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.multi_headed_self_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.feed_forward = MultiLayerPerceptron(dense_dim, embedding_dim, dropout_p)\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "\n",
    "        multi_headed_self_attention_output = self.multi_headed_self_attention(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "        )\n",
    "        feed_forward_input = self.layernorm_1(inputs + multi_headed_self_attention_output)\n",
    "        feed_forward_output = self.feed_forward(feed_forward_input)\n",
    "        out = self.layernorm_2(feed_forward_input + feed_forward_output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(Layer):\n",
    "\n",
    "    def __init__(self, embedding_dim, dense_dim, num_heads, dropout_p, **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.masked_multi_headed_self_attention = attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.multi_headed_cross_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "        self.feed_forward = MultiLayerPerceptron(dense_dim, embedding_dim, dropout_p)\n",
    "        self.layernorm_3 = LayerNormalization()\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "\n",
    "        causal_mask = generate_self_attention_mask(inputs)\n",
    "        masked_multi_headed_self_attention_output = self.masked_multi_headed_self_attention(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask    \n",
    "        )\n",
    "        masked_multi_headed_self_attention_output_normalized = self.layernorm_1(inputs + masked_multi_headed_self_attention_output)\n",
    "\n",
    "        multi_headed_cross_attention_output = self.multi_headed_cross_attention(\n",
    "            query=masked_multi_headed_self_attention_output_normalized,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "        )\n",
    "\n",
    "        feed_forward_input = self.layernorm_2(masked_multi_headed_self_attention_output + masked_multi_headed_self_attention_output_normalized)\n",
    "        feed_forward_output = self.feed_forward(feed_forward_input)\n",
    "        out = self.layernorm_3(feed_forward_input + feed_forward_output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "dropout_p = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "x = EmbeddingBlock(english_vocab_size, embedding_dim, english_sequence_len)(encoder_inputs)\n",
    "encoder_block_outputs = EncoderBlock(embedding_dim, dense_dim, num_heads, dropout_p)(x)\n",
    "\n",
    "encoder = Model(encoder_inputs, encoder_block_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = Input(shape=(None, embedding_dim), name=\"encoded_seq_inputs\")\n",
    "\n",
    "x = EmbeddingBlock(spanish_vocab_size, embedding_dim, spanish_sequence_len)(decoder_inputs)\n",
    "x = DecoderBlock(embedding_dim, dense_dim, num_heads, dropout_p)(x, encoded_seq_inputs)\n",
    "x = Dropout(dropout_p)(x)\n",
    "decoder_block_outputs = Dense(spanish_vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "decoder = Model([decoder_inputs, encoded_seq_inputs], decoder_block_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_inputs = [encoder_inputs, decoder_inputs]\n",
    "transformer_outputs = decoder([decoder_inputs, encoder_block_outputs])\n",
    "\n",
    "transformer = Model(\n",
    "    transformer_inputs, transformer_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_block_2 (EmbeddingBl  (None, None, 256)   3223296     ['encoder_inputs[0][0]']         \n",
      " ock)                                                                                             \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " encoder_block_1 (EncoderBlock)  (None, None, 256)   3155456     ['embedding_block_2[0][0]']      \n",
      "                                                                                                  \n",
      " model_3 (Functional)           (None, None, 37345)  24421857    ['decoder_inputs[0][0]',         \n",
      "                                                                  'encoder_block_1[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 30,800,609\n",
      "Trainable params: 30,800,609\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    # \"rmsprop\",\n",
    "    'adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_block_2/embedding_4/embeddings:0', 'embedding_block_2/embedding_5/embeddings:0', 'encoder_block_1/multi_head_attention_3/query/kernel:0', 'encoder_block_1/multi_head_attention_3/query/bias:0', 'encoder_block_1/multi_head_attention_3/key/kernel:0', 'encoder_block_1/multi_head_attention_3/key/bias:0', 'encoder_block_1/multi_head_attention_3/value/kernel:0', 'encoder_block_1/multi_head_attention_3/value/bias:0', 'encoder_block_1/multi_head_attention_3/attention_output/kernel:0', 'encoder_block_1/multi_head_attention_3/attention_output/bias:0', 'encoder_block_1/layer_normalization_5/gamma:0', 'encoder_block_1/layer_normalization_5/beta:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'encoder_block_1/layer_normalization_6/gamma:0', 'encoder_block_1/layer_normalization_6/beta:0', 'decoder_block_1/multi_head_attention_5/query/kernel:0', 'decoder_block_1/multi_head_attention_5/query/bias:0', 'decoder_block_1/multi_head_attention_5/key/kernel:0', 'decoder_block_1/multi_head_attention_5/key/bias:0', 'decoder_block_1/multi_head_attention_5/value/kernel:0', 'decoder_block_1/multi_head_attention_5/value/bias:0', 'decoder_block_1/multi_head_attention_5/attention_output/kernel:0', 'decoder_block_1/multi_head_attention_5/attention_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_block_2/embedding_4/embeddings:0', 'embedding_block_2/embedding_5/embeddings:0', 'encoder_block_1/multi_head_attention_3/query/kernel:0', 'encoder_block_1/multi_head_attention_3/query/bias:0', 'encoder_block_1/multi_head_attention_3/key/kernel:0', 'encoder_block_1/multi_head_attention_3/key/bias:0', 'encoder_block_1/multi_head_attention_3/value/kernel:0', 'encoder_block_1/multi_head_attention_3/value/bias:0', 'encoder_block_1/multi_head_attention_3/attention_output/kernel:0', 'encoder_block_1/multi_head_attention_3/attention_output/bias:0', 'encoder_block_1/layer_normalization_5/gamma:0', 'encoder_block_1/layer_normalization_5/beta:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'encoder_block_1/layer_normalization_6/gamma:0', 'encoder_block_1/layer_normalization_6/beta:0', 'decoder_block_1/multi_head_attention_5/query/kernel:0', 'decoder_block_1/multi_head_attention_5/query/bias:0', 'decoder_block_1/multi_head_attention_5/key/kernel:0', 'decoder_block_1/multi_head_attention_5/key/bias:0', 'decoder_block_1/multi_head_attention_5/value/kernel:0', 'decoder_block_1/multi_head_attention_5/value/bias:0', 'decoder_block_1/multi_head_attention_5/attention_output/kernel:0', 'decoder_block_1/multi_head_attention_5/attention_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_block_2/embedding_4/embeddings:0', 'embedding_block_2/embedding_5/embeddings:0', 'encoder_block_1/multi_head_attention_3/query/kernel:0', 'encoder_block_1/multi_head_attention_3/query/bias:0', 'encoder_block_1/multi_head_attention_3/key/kernel:0', 'encoder_block_1/multi_head_attention_3/key/bias:0', 'encoder_block_1/multi_head_attention_3/value/kernel:0', 'encoder_block_1/multi_head_attention_3/value/bias:0', 'encoder_block_1/multi_head_attention_3/attention_output/kernel:0', 'encoder_block_1/multi_head_attention_3/attention_output/bias:0', 'encoder_block_1/layer_normalization_5/gamma:0', 'encoder_block_1/layer_normalization_5/beta:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'encoder_block_1/layer_normalization_6/gamma:0', 'encoder_block_1/layer_normalization_6/beta:0', 'decoder_block_1/multi_head_attention_5/query/kernel:0', 'decoder_block_1/multi_head_attention_5/query/bias:0', 'decoder_block_1/multi_head_attention_5/key/kernel:0', 'decoder_block_1/multi_head_attention_5/key/bias:0', 'decoder_block_1/multi_head_attention_5/value/kernel:0', 'decoder_block_1/multi_head_attention_5/value/bias:0', 'decoder_block_1/multi_head_attention_5/attention_output/kernel:0', 'decoder_block_1/multi_head_attention_5/attention_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_block_2/embedding_4/embeddings:0', 'embedding_block_2/embedding_5/embeddings:0', 'encoder_block_1/multi_head_attention_3/query/kernel:0', 'encoder_block_1/multi_head_attention_3/query/bias:0', 'encoder_block_1/multi_head_attention_3/key/kernel:0', 'encoder_block_1/multi_head_attention_3/key/bias:0', 'encoder_block_1/multi_head_attention_3/value/kernel:0', 'encoder_block_1/multi_head_attention_3/value/bias:0', 'encoder_block_1/multi_head_attention_3/attention_output/kernel:0', 'encoder_block_1/multi_head_attention_3/attention_output/bias:0', 'encoder_block_1/layer_normalization_5/gamma:0', 'encoder_block_1/layer_normalization_5/beta:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'encoder_block_1/layer_normalization_6/gamma:0', 'encoder_block_1/layer_normalization_6/beta:0', 'decoder_block_1/multi_head_attention_5/query/kernel:0', 'decoder_block_1/multi_head_attention_5/query/bias:0', 'decoder_block_1/multi_head_attention_5/key/kernel:0', 'decoder_block_1/multi_head_attention_5/key/bias:0', 'decoder_block_1/multi_head_attention_5/value/kernel:0', 'decoder_block_1/multi_head_attention_5/value/bias:0', 'decoder_block_1/multi_head_attention_5/attention_output/kernel:0', 'decoder_block_1/multi_head_attention_5/attention_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "2975/2975 [==============================] - 524s 176ms/step - loss: 2.0731 - accuracy: 0.6833 - val_loss: 1.8516 - val_accuracy: 0.6968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e57b3f70>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3bfd8a20af1d5d4753b5a38df58ddc27d816f3a7f519b02fcfafca237454594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
