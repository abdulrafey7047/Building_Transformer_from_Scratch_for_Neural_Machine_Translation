{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as platform\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import (\n",
    "    Layer, Dense, Embedding, Dropout, Input,\n",
    "    MultiHeadAttention, LayerNormalization,\n",
    ")\n",
    "\n",
    "## To replicate the results\n",
    "from tensorflow.random import set_seed\n",
    "from numpy.random import seed\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)\n",
    "seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "## If want to train on CPU instead of GPU\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_df(df, data):\n",
    "\n",
    "    english_lines = list()\n",
    "    spanish_lines = list()\n",
    "    for data_line in data:\n",
    "        english_line, spanish_line = data_line.split(\"\\t\")\n",
    "\n",
    "        english_lines.append(english_line)\n",
    "        spanish_lines.append(spanish_line)\n",
    "    \n",
    "    df['english'] = english_lines\n",
    "    df['spanish'] = spanish_lines\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_self_attention_mask(inputs):\n",
    "\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "\n",
    "        self_attention_mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        self_attention_mask = tf.reshape(self_attention_mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(self_attention_mask, mult)\n",
    "\n",
    "\n",
    "def pad_and_tokenize(line, tokenizer, max_len):\n",
    "\n",
    "    line = pad_sequences(\n",
    "        tokenizer.texts_to_sequences(np.expand_dims(line, axis=0)),\n",
    "        maxlen=max_len,\n",
    "        padding='post',\n",
    "    )\n",
    "\n",
    "    return np.squeeze(line).tolist()\n",
    "\n",
    "\n",
    "def format_dataset(english_line, spanish_line):\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": english_line,\n",
    "            \"decoder_inputs\": spanish_line[:, :-1],\n",
    "        },\n",
    "        spanish_line[:, 1:]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_sentence(tokens, index_lookup):\n",
    "\n",
    "    sentence = list()\n",
    "    for token in tokens:\n",
    "        word = index_lookup[token]\n",
    "        sentence.append(word)\n",
    "\n",
    "    return ' '.join(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = get_file(\n",
    "    fname=\"dataset.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file_path = f\"{'/'.join(dataset_path.split('/')[:-1])}/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_file_path) as f:\n",
    "    data = f.read().split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['english', 'spanish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = populate_df(df, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting to lowercase\n",
    "\n",
    "df['english'] = df['english'].apply(str.lower)\n",
    "df['spanish'] = df['spanish'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replacing puncutation\n",
    "\n",
    "df['english'] = df['english'].apply(lambda line: re.sub(r'[^\\w\\s]', '', line))\n",
    "df['spanish'] = df['spanish'].apply(lambda line: re.sub(r'[^\\w\\s]' + \"Â¿\", '', line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding <START> and <END> Token\n",
    "\n",
    "df['spanish'] = df['spanish'].apply(\n",
    "    lambda data_line: f'<START> {data_line} <END>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = df.shape[0]\n",
    "\n",
    "print(f'There are {num_examples} training examples in the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(num_examples * 0.94)\n",
    "val_size = int(num_examples * 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:train_size+val_size]\n",
    "test_df = df.iloc[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'There are:\\n\\t{train_df.shape[0]} training examples\\n\\t{val_df.shape[0]} validation examples\\n\\t{test_df.shape[0]} test examples'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "spanish_tokenizer = Tokenizer(oov_token='<OOV>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer.fit_on_texts(train_df['english'])\n",
    "spanish_tokenizer.fit_on_texts(train_df['spanish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding one for PADDING token\n",
    "\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "spanish_vocab_size = len(spanish_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_lookup = english_tokenizer.word_index\n",
    "spanish_word_lookup = spanish_tokenizer.word_index\n",
    "\n",
    "english_token_lookup = { value: key for key, value in english_word_lookup.items()}\n",
    "spanish_token_lookup = { value: key for key, value in spanish_word_lookup.items()}\n",
    "\n",
    "english_token_lookup[0] = ''\n",
    "spanish_token_lookup[0] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentence_lengths = train_df['english'].map(str.split).map(len)\n",
    "spanish_sentence_lengths = train_df['spanish'].map(str.split).map(len)\n",
    "\n",
    "print(f'''\n",
    "    English Sentence Length Stats:\n",
    "    \\tMaximum Headline length: {english_sentence_lengths.max()}\n",
    "    \\tMinimum Headline length: {english_sentence_lengths.min()}\n",
    "    \\tAverage Headline length: {english_sentence_lengths.mean():.2f}\n",
    "    \\tSTD of Headline length: {english_sentence_lengths.std():.2f}\n",
    "\n",
    "    Spanish Sentence Length Stats:\n",
    "    \\tMaximum Headline length: {spanish_sentence_lengths.max()}\n",
    "    \\tMinimum Headline length: {spanish_sentence_lengths.min()}\n",
    "    \\tAverage Headline length: {spanish_sentence_lengths.mean():.2f}\n",
    "    \\tSTD of Headline length: {spanish_sentence_lengths.std():.2f}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sequence_len = math.ceil(english_sentence_lengths.mean() + (3 * english_sentence_lengths.std()))\n",
    "spanish_sequence_len = math.ceil(spanish_sentence_lengths.mean() + (3 * spanish_sentence_lengths.std()))\n",
    "\n",
    "\n",
    "english_sequence_len, spanish_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['english'] = train_df['english'].map(\n",
    "    lambda english_line: pad_and_tokenize(\n",
    "        english_line,\n",
    "        english_tokenizer,\n",
    "        english_sequence_len\n",
    "        )\n",
    ")\n",
    "\n",
    "train_df['spanish'] = train_df['spanish'].map(\n",
    "    lambda spanish_line: pad_and_tokenize(\n",
    "        spanish_line,\n",
    "        spanish_tokenizer,\n",
    "        spanish_sequence_len+1,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['english'] = val_df['english'].map(\n",
    "    lambda english_line: pad_and_tokenize(\n",
    "        english_line,\n",
    "        english_tokenizer,\n",
    "        english_sequence_len\n",
    "        )\n",
    ")\n",
    "\n",
    "val_df['spanish'] = val_df['spanish'].map(\n",
    "    lambda spanish_line: pad_and_tokenize(\n",
    "        spanish_line,\n",
    "        spanish_tokenizer,\n",
    "        spanish_sequence_len+1,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['english'] = test_df['english'].map(\n",
    "    lambda english_line: pad_and_tokenize(\n",
    "        english_line,\n",
    "        english_tokenizer,\n",
    "        english_sequence_len\n",
    "        )\n",
    ")\n",
    "\n",
    "test_df['spanish'] = test_df['spanish'].map(\n",
    "    lambda spanish_line: pad_and_tokenize(\n",
    "        spanish_line,\n",
    "        spanish_tokenizer,\n",
    "        spanish_sequence_len+1,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets from Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.zip((\n",
    "    Dataset.from_tensor_slices(train_df['english'].tolist()),\n",
    "    Dataset.from_tensor_slices(train_df['spanish'].tolist()))).batch(\n",
    "        batch_size).map(\n",
    "            format_dataset).shuffle(\n",
    "                train_df['english'].shape[0]).prefetch(\n",
    "                batch_size).cache()\n",
    "\n",
    "val_ds = Dataset.zip((\n",
    "    Dataset.from_tensor_slices(val_df['english'].tolist()),\n",
    "    Dataset.from_tensor_slices(val_df['spanish'].tolist()))).batch(\n",
    "        batch_size).map(\n",
    "            format_dataset).shuffle(\n",
    "                val_df['english'].shape[0]).prefetch(\n",
    "                batch_size).cache()\n",
    "\n",
    "test_ds = Dataset.zip((\n",
    "    Dataset.from_tensor_slices(test_df['english'].tolist()),\n",
    "    Dataset.from_tensor_slices(test_df['spanish'].tolist()))).batch(\n",
    "        1).map(\n",
    "            format_dataset).shuffle(\n",
    "                test_df['english'].shape[0]).prefetch(\n",
    "                1).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in train_ds.take(1):\n",
    "    print(X)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Architecture of Blocks of Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim,  sequence_length, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.token_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.position_embeddings = Embedding(input_dim=sequence_length, output_dim=embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        positions = tf.range(start=0, limit=tf.shape(inputs)[-1], delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        out = embedded_tokens + embedded_positions\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(Layer):\n",
    "\n",
    "    def __init__(self, dense_dim, out_dim, dropout_p, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.dense = Sequential([\n",
    "            Dense(dense_dim, activation='relu'), \n",
    "            Dropout(dropout_p),\n",
    "            Dense(out_dim, activation='relu'),\n",
    "            Dropout(dropout_p),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(Layer):\n",
    "\n",
    "    def __init__(self, embedding_dim, dense_dim, num_heads, dropout_p, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.multi_headed_self_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.feed_forward = MultiLayerPerceptron(dense_dim, embedding_dim, dropout_p)\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        multi_headed_self_attention_output = self.multi_headed_self_attention(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "        )\n",
    "        feed_forward_input = self.layernorm_1(inputs + multi_headed_self_attention_output)\n",
    "        feed_forward_output = self.feed_forward(feed_forward_input)\n",
    "        out = self.layernorm_2(feed_forward_input + feed_forward_output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(Layer):\n",
    "\n",
    "    def __init__(self, embedding_dim, dense_dim, num_heads, dropout_p, **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.masked_multi_headed_self_attention = attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.multi_headed_cross_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "        self.feed_forward = MultiLayerPerceptron(dense_dim, embedding_dim, dropout_p)\n",
    "        self.layernorm_3 = LayerNormalization()\n",
    "        self.dropout = Dropout(dropout_p)\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs):\n",
    "\n",
    "        causal_mask = generate_self_attention_mask(inputs)\n",
    "        masked_multi_headed_self_attention_output = self.masked_multi_headed_self_attention(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask    \n",
    "        )\n",
    "        masked_multi_headed_self_attention_output_normalized = self.layernorm_1(inputs + masked_multi_headed_self_attention_output)\n",
    "        masked_multi_headed_self_attention_output_normalized = self.dropout(masked_multi_headed_self_attention_output_normalized)\n",
    "\n",
    "        multi_headed_cross_attention_output = self.multi_headed_cross_attention(\n",
    "            query=masked_multi_headed_self_attention_output_normalized,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "        )\n",
    "\n",
    "        feed_forward_input = self.layernorm_2(masked_multi_headed_self_attention_output + masked_multi_headed_self_attention_output_normalized)\n",
    "        feed_forward_output = self.feed_forward(feed_forward_input)\n",
    "\n",
    "        out = self.layernorm_3(feed_forward_input + feed_forward_output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "dropout_p = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "x = EmbeddingBlock(english_vocab_size, embedding_dim, english_sequence_len)(encoder_inputs)\n",
    "encoder_block_outputs = EncoderBlock(embedding_dim, dense_dim, num_heads, dropout_p)(x)\n",
    "\n",
    "encoder = Model(encoder_inputs, encoder_block_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = Input(shape=(None, embedding_dim), name=\"encoded_seq_inputs\")\n",
    "\n",
    "x = EmbeddingBlock(spanish_vocab_size, embedding_dim, spanish_sequence_len)(decoder_inputs)\n",
    "x = DecoderBlock(embedding_dim, dense_dim, num_heads, dropout_p)(x, encoded_seq_inputs)\n",
    "x = Dropout(dropout_p)(x)\n",
    "decoder_block_outputs = Dense(spanish_vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "decoder = Model([decoder_inputs, encoded_seq_inputs], decoder_block_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_inputs = [encoder_inputs, decoder_inputs]\n",
    "transformer_outputs = decoder([decoder_inputs, encoder_block_outputs])\n",
    "\n",
    "transformer = Model(\n",
    "    transformer_inputs, transformer_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay(epoch):\n",
    "        \n",
    "   initial_lrate = 1e-4\n",
    "   drop = 0.5\n",
    "   epochs_drop = 10.0\n",
    "   lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "   return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    optimizer=Adam(learning_rate=(1e-4)),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='model-epoch10+{epoch:02d}-loss{val_loss:.2f}.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1, \n",
    "    save_best_only=True,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[checkpoint, lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model-epoch10+01-loss1.78.h5'\n",
    "\n",
    "transformer = load_model(model_path, custom_objects={\n",
    "    'EmbeddingBlock': EmbeddingBlock,\n",
    "    'MultiLayerPerceptron': MultiLayerPerceptron,\n",
    "    'EncoderBlock': EncoderBlock,\n",
    "    'DecoderBlock': DecoderBlock\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence, model,  max_output_len, spanish_token_lookup, word_sample_size):\n",
    "\n",
    "    translated_sentence = '<START>'\n",
    "    for i in range(max_output_len):\n",
    "\n",
    "        tokenized_target_sentence = pad_and_tokenize(\n",
    "            translated_sentence,\n",
    "            spanish_tokenizer,\n",
    "            spanish_sequence_len\n",
    "        )[:-1]\n",
    "\n",
    "        encoder_inputs = input_sentence['encoder_inputs']\n",
    "        decoder_inputs = np.expand_dims(tokenized_target_sentence, axis=0)\n",
    "\n",
    "        predictions = model([encoder_inputs, decoder_inputs])[0]\n",
    "        \n",
    "        top_n_pred_tokens = np.argpartition(predictions[i, :], -word_sample_size)[-word_sample_size:]\n",
    "        pred_token = np.random.choice(top_n_pred_tokens, size=1)[0]\n",
    "\n",
    "        if pred_token:\n",
    "            sampled_word = spanish_token_lookup[pred_token]\n",
    "            translated_sentence += ' ' + sampled_word\n",
    "\n",
    "        if pred_token == '<END>':\n",
    "            break\n",
    "    else:\n",
    "        translated_sentence += ' <END>'\n",
    "\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X, y in test_ds.take(3):\n",
    "\n",
    "#     actual_eng = get_sentence(X['encoder_inputs'][0].numpy(), english_token_lookup)\n",
    "#     translated = translate(X, transformer, spanish_sequence_len-1, spanish_token_lookup, 2)\n",
    "\n",
    "#     print('ACTUAL:', actual_eng)\n",
    "#     print('TRANSLATED:', translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3bfd8a20af1d5d4753b5a38df58ddc27d816f3a7f519b02fcfafca237454594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
