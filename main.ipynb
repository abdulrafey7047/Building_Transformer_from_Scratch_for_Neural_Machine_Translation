{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as platform\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import (\n",
    "    Layer, Dense, Embedding, Dropout, Input,\n",
    "    MultiHeadAttention, LayerNormalization,\n",
    ")\n",
    "\n",
    "## To replicate the results\n",
    "from tensorflow.random import set_seed\n",
    "from numpy.random import seed\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)\n",
    "seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "## If want to train on CPU instead of GPU\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_df(df, data):\n",
    "\n",
    "    english_lines = list()\n",
    "    spanish_lines = list()\n",
    "    for data_line in data:\n",
    "        english_line, spanish_line = data_line.split(\"\\t\")\n",
    "\n",
    "        english_lines.append(english_line)\n",
    "        spanish_lines.append(spanish_line)\n",
    "    \n",
    "    df['english'] = english_lines\n",
    "    df['spanish'] = spanish_lines\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_self_attention_mask(inputs):\n",
    "\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "\n",
    "        self_attention_mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        self_attention_mask = tf.reshape(self_attention_mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(self_attention_mask, mult)\n",
    "\n",
    "\n",
    "# def generate_self_attention_mask(inputs):\n",
    "\n",
    "#     batch_size, sequence_len = np.shape(inputs)[:2]\n",
    "\n",
    "#     mask =  np.expand_dims(\n",
    "#                 np.tri(\n",
    "#                     sequence_len, sequence_len, dtype=np.int32),\n",
    "#                 axis=0\n",
    "#             )\n",
    "\n",
    "#     return mask.repeat(batch_size, axis=0)\n",
    "\n",
    "\n",
    "def pad_and_tokenize(line, tokenizer, max_len):\n",
    "\n",
    "    line = pad_sequences(\n",
    "        tokenizer.texts_to_sequences(np.expand_dims(line, axis=0)),\n",
    "        maxlen=max_len,\n",
    "        padding='post',\n",
    "    )\n",
    "\n",
    "    return np.squeeze(line).tolist()\n",
    "\n",
    "\n",
    "def format_dataset(english_line, spanish_line):\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": english_line,\n",
    "            \"decoder_inputs\": spanish_line[:, :-1],\n",
    "        },\n",
    "        spanish_line[:, 1:]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_sentence(tokens, index_lookup):\n",
    "\n",
    "    sentence = list()\n",
    "    for token in tokens:\n",
    "        word = index_lookup[token]\n",
    "        sentence.append(word)\n",
    "\n",
    "    return ' '.join(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = get_file(\n",
    "    fname=\"dataset.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file_path = f\"{'/'.join(dataset_path.split('/')[:-1])}/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_file_path) as f:\n",
    "    data = f.read().split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['english', 'spanish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = populate_df(df, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english  spanish\n",
       "0     Go.      Ve.\n",
       "1     Go.    Vete.\n",
       "2     Go.    Vaya.\n",
       "3     Go.  Váyase.\n",
       "4     Hi.    Hola."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting to lowercase\n",
    "\n",
    "df['english'] = df['english'].apply(str.lower)\n",
    "df['spanish'] = df['spanish'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replacing puncutation\n",
    "\n",
    "df['english'] = df['english'].apply(lambda line: re.sub(r'[^\\w\\s]', '', line))\n",
    "df['spanish'] = df['spanish'].apply(lambda line: re.sub(r'[^\\w\\s]' + \"¿\", '', line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding <START> and <END> Token\n",
    "\n",
    "df['spanish'] = df['spanish'].apply(\n",
    "    lambda data_line: f'<START> {data_line} <END>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>&lt;START&gt; ve. &lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go</td>\n",
       "      <td>&lt;START&gt; vete. &lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>go</td>\n",
       "      <td>&lt;START&gt; vaya. &lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>go</td>\n",
       "      <td>&lt;START&gt; váyase. &lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi</td>\n",
       "      <td>&lt;START&gt; hola. &lt;END&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english                spanish\n",
       "0      go      <START> ve. <END>\n",
       "1      go    <START> vete. <END>\n",
       "2      go    <START> vaya. <END>\n",
       "3      go  <START> váyase. <END>\n",
       "4      hi    <START> hola. <END>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 118964 training examples in the data\n"
     ]
    }
   ],
   "source": [
    "num_examples = df.shape[0]\n",
    "\n",
    "print(f'There are {num_examples} training examples in the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(num_examples * 0.94)\n",
    "val_size = int(num_examples * 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:train_size+val_size]\n",
    "test_df = df.iloc[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:\n",
      "\t111826 training examples\n",
      "\t3568 validation examples\n",
      "\t3570 test examples\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'There are:\\n\\t{train_df.shape[0]} training examples\\n\\t{val_df.shape[0]} validation examples\\n\\t{test_df.shape[0]} test examples'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "spanish_tokenizer = Tokenizer(oov_token='<OOV>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer.fit_on_texts(train_df['english'])\n",
    "spanish_tokenizer.fit_on_texts(train_df['spanish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding one for PADDING token\n",
    "\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "spanish_vocab_size = len(spanish_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_lookup = english_tokenizer.word_index\n",
    "spanish_word_lookup = spanish_tokenizer.word_index\n",
    "\n",
    "english_token_lookup = { value: key for key, value in english_word_lookup.items()}\n",
    "spanish_token_lookup = { value: key for key, value in spanish_word_lookup.items()}\n",
    "\n",
    "english_token_lookup[0] = ''\n",
    "spanish_token_lookup[0] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    English Sentence Length Stats:\n",
      "    \tMaximum Headline length: 47\n",
      "    \tMinimum Headline length: 1\n",
      "    \tAverage Headline length: 6.31\n",
      "    \tSTD of Headline length: 2.61\n",
      "\n",
      "    Spanish Sentence Length Stats:\n",
      "    \tMaximum Headline length: 51\n",
      "    \tMinimum Headline length: 3\n",
      "    \tAverage Headline length: 8.09\n",
      "    \tSTD of Headline length: 2.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "english_sentence_lengths = train_df['english'].map(str.split).map(len)\n",
    "spanish_sentence_lengths = train_df['spanish'].map(str.split).map(len)\n",
    "\n",
    "print(f'''\n",
    "    English Sentence Length Stats:\n",
    "    \\tMaximum Headline length: {english_sentence_lengths.max()}\n",
    "    \\tMinimum Headline length: {english_sentence_lengths.min()}\n",
    "    \\tAverage Headline length: {english_sentence_lengths.mean():.2f}\n",
    "    \\tSTD of Headline length: {english_sentence_lengths.std():.2f}\n",
    "\n",
    "    Spanish Sentence Length Stats:\n",
    "    \\tMaximum Headline length: {spanish_sentence_lengths.max()}\n",
    "    \\tMinimum Headline length: {spanish_sentence_lengths.min()}\n",
    "    \\tAverage Headline length: {spanish_sentence_lengths.mean():.2f}\n",
    "    \\tSTD of Headline length: {spanish_sentence_lengths.std():.2f}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 17)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sequence_len = math.ceil(english_sentence_lengths.mean() + (3 * english_sentence_lengths.std()))\n",
    "spanish_sequence_len = math.ceil(spanish_sentence_lengths.mean() + (3 * spanish_sentence_lengths.std()))\n",
    "\n",
    "\n",
    "english_sequence_len, spanish_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['english'] = train_df['english'].map(\n",
    "    lambda english_line: pad_and_tokenize(\n",
    "        english_line,\n",
    "        english_tokenizer,\n",
    "        english_sequence_len\n",
    "        )\n",
    ")\n",
    "\n",
    "train_df['spanish'] = train_df['spanish'].map(\n",
    "    lambda spanish_line: pad_and_tokenize(\n",
    "        spanish_line,\n",
    "        spanish_tokenizer,\n",
    "        spanish_sequence_len+1,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['english'] = val_df['english'].map(\n",
    "    lambda english_line: pad_and_tokenize(\n",
    "        english_line,\n",
    "        english_tokenizer,\n",
    "        english_sequence_len\n",
    "        )\n",
    ")\n",
    "\n",
    "val_df['spanish'] = val_df['spanish'].map(\n",
    "    lambda spanish_line: pad_and_tokenize(\n",
    "        spanish_line,\n",
    "        spanish_tokenizer,\n",
    "        spanish_sequence_len+1,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['english'] = test_df['english'].map(\n",
    "    lambda english_line: pad_and_tokenize(\n",
    "        english_line,\n",
    "        english_tokenizer,\n",
    "        english_sequence_len\n",
    "        )\n",
    ")\n",
    "\n",
    "test_df['spanish'] = test_df['spanish'].map(\n",
    "    lambda spanish_line: pad_and_tokenize(\n",
    "        spanish_line,\n",
    "        spanish_tokenizer,\n",
    "        spanish_sequence_len+1,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets from Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.zip((\n",
    "    Dataset.from_tensor_slices(train_df['english'].tolist()),\n",
    "    Dataset.from_tensor_slices(train_df['spanish'].tolist()))).batch(\n",
    "        batch_size).map(\n",
    "            format_dataset).shuffle(\n",
    "                train_df['english'].shape[0]).prefetch(\n",
    "                batch_size).cache()\n",
    "\n",
    "val_ds = Dataset.zip((\n",
    "    Dataset.from_tensor_slices(val_df['english'].tolist()),\n",
    "    Dataset.from_tensor_slices(val_df['spanish'].tolist()))).batch(\n",
    "        batch_size).map(\n",
    "            format_dataset).shuffle(\n",
    "                val_df['english'].shape[0]).prefetch(\n",
    "                batch_size).cache()\n",
    "\n",
    "test_ds = Dataset.zip((\n",
    "    Dataset.from_tensor_slices(test_df['english'].tolist()),\n",
    "    Dataset.from_tensor_slices(test_df['spanish'].tolist()))).batch(\n",
    "        1).map(\n",
    "            format_dataset).shuffle(\n",
    "                test_df['english'].shape[0]).prefetch(\n",
    "                1).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 12:30:09.430702: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoder_inputs': <tf.Tensor: shape=(32, 15), dtype=int32, numpy=\n",
      "array([[    6,  4591,    12,     9,    55,   138,     7,   460,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [    6,    44,   129,    53,   228,    53,     9,   548,   111,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [   34,  1231,    12,    80,   147,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [  108,     5,    86,    16,    49,     4,    61,     4,  1397,\n",
      "          427,     0,     0,     0,     0,     0],\n",
      "       [    9,   138,    16,   999,    25,  2206,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [    3,  1132,     4,    88,    39,    10,    25,   109,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [    3,    55,     7,  5035,   100,   240,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [   80,     7,   321,   150,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [    3,   319,   933,   149,   104,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [    9,  1424,    62,     2,  3013,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [   21,     8,    69,   832,   252,    92,    42,   336,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [   23,     5,   157,  1510,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [  222,    28,   953,   768,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [   34,  1571,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [    3,    60,    45,    48,    20,    16,     4,  1564,  2361,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [    6,   156,     4,  1782,     2,  2695,    89,   205,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [    2,   687,    14,    99,   974,    20,    16,     4,   999,\n",
      "           72,   433,     0,     0,     0,     0],\n",
      "       [ 3893,    14,  1089,    10,  7412,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [   40,     3,   219,    19,   236,    10,   310,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [   42,  5402,     8, 11407,    89,    21,   421,     4,   200,\n",
      "           31,    87,  3599,     0,     0,     0],\n",
      "       [    3,    22,    38,     2,  3886,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [   17,     8,   354,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [   47,    54,  1028,    58,    37,     2,   274,    48,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [    3,   108,    33,   208,    24,     2,  1062,   116,   486,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [    6,   550,   746,     7,  1073,   501,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [    3,   200,  2688,     7,   124,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [  105,   269,   189,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [   63,   127,    13,   402,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [ 1114,  6561,     5,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [   49,    51,     5, 11408,    17,  1091,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [  299,    71,     4,    30,    10,  1026,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0],\n",
      "       [   80,    33,  1303,    23,     5,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]], dtype=int32)>, 'decoder_inputs': <tf.Tensor: shape=(32, 17), dtype=int32, numpy=\n",
      "array([[    2,     8,  3426,     5,   100,  1615,    13,   417,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    92, 13417,    60,   202,    52,  1097,     6,    50,\n",
      "            3,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    37,  1921,     4,     5,   121,   375,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,   281,   251,  6329,   199,   351,     6,     9,   366,\n",
      "            4,  1595,     3,     0,     0,     0,     0,     0],\n",
      "       [    2,    16,    14,   124,  3605,    20,  1973,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,  6836,  2024,    24,    20,    83,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,   399,    13,  7240,    10,   209,   183,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,   131,    13,   144,   919,     3,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    63,    21,   151,  1240,  1343,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    16,  4473,     9,  4164,     3,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    25,    12,    33,   213,    60,  1199,    52,    20,\n",
      "          277,     3,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,  1494,   849,   453,     3,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,  8359,     9,   812,     3,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    37,  4878,     3,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    86,     5,    12,   132,     4,     5,    14,   456,\n",
      "            6,    13,   367,     3,     0,     0,     0,     0],\n",
      "       [    2,     8,  1284,  7256,    10,  3495,    90,     7,   292,\n",
      "            3,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     9,  1061,    67,    38,  3040,    28, 13201,    34,\n",
      "           79,     3,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,  8237,    46,  3369,    11, 12019,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,   241,   786,     6,    22,   221,    11,   261,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    20,  8032,    12,  1602,    90,    25,   363,   654,\n",
      "           24,    32,  4475,     3,     0,     0,     0,     0],\n",
      "       [    2,    34,     7,   388,     6,     9,   605,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    56,    12,  2249,     3,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    63,    15,  6132,     6,     9,    70,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     7,  3394,    17,     5,    88,    10,  1028,   693,\n",
      "            3,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     8,     7,    15,    58,   495,   220,    18,   401,\n",
      "          263,     3,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,   505,    73,    33,  4041,     3,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    21,   248,     3,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,   204,  1025,     3,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,  4956,  1099,    31, 14406,     3,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,   146,  2508,   135,    41,  1197,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,    25,   717,     6,  1485,     3,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [    2,     7,   115,  1493,   386,     3,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "      dtype=int32)>}\n",
      "tf.Tensor(\n",
      "[[    8  3426     5   100  1615    13   417     3     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   92 13417    60   202    52  1097     6    50     3     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   37  1921     4     5   121   375     3     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [  281   251  6329   199   351     6     9   366     4  1595     3     0\n",
      "      0     0     0     0     0]\n",
      " [   16    14   124  3605    20  1973     3     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [ 6836  2024    24    20    83     3     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [  399    13  7240    10   209   183     3     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [  131    13   144   919     3     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   63    21   151  1240  1343     3     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   16  4473     9  4164     3     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   25    12    33   213    60  1199    52    20   277     3     0     0\n",
      "      0     0     0     0     0]\n",
      " [ 1494   849   453     3     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [ 8359     9   812     3     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   37  4878     3     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   86     5    12   132     4     5    14   456     6    13   367     3\n",
      "      0     0     0     0     0]\n",
      " [    8  1284  7256    10  3495    90     7   292     3     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    9  1061    67    38  3040    28 13201    34    79     3     0     0\n",
      "      0     0     0     0     0]\n",
      " [ 8237    46  3369    11 12019     3     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [  241   786     6    22   221    11   261     3     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   20  8032    12  1602    90    25   363   654    24    32  4475     3\n",
      "      0     0     0     0     0]\n",
      " [   34     7   388     6     9   605     3     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   56    12  2249     3     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   63    15  6132     6     9    70     3     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    7  3394    17     5    88    10  1028   693     3     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    8     7    15    58   495   220    18   401   263     3     0     0\n",
      "      0     0     0     0     0]\n",
      " [  505    73    33  4041     3     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   21   248     3     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [  204  1025     3     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [ 4956  1099    31 14406     3     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [  146  2508   135    41  1197     3     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   25   717     6  1485     3     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    7   115  1493   386     3     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]], shape=(32, 17), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 12:30:09.633380: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_ds.take(1):\n",
    "    print(X)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Architecture of Blocks of Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim,  sequence_length, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.token_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.position_embeddings = Embedding(input_dim=sequence_length, output_dim=embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        positions = tf.range(start=0, limit=tf.shape(inputs)[-1], delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        out = embedded_tokens + embedded_positions\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(Layer):\n",
    "\n",
    "    def __init__(self, dense_dim, out_dim, dropout_p, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.dense = Sequential([\n",
    "            Dense(dense_dim, activation='relu'), \n",
    "            Dropout(dropout_p),\n",
    "            Dense(out_dim, activation='relu'),\n",
    "            Dropout(dropout_p),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(Layer):\n",
    "\n",
    "    def __init__(self, embedding_dim, dense_dim, num_heads, dropout_p, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.multi_headed_self_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.feed_forward = MultiLayerPerceptron(dense_dim, embedding_dim, dropout_p)\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        multi_headed_self_attention_output = self.multi_headed_self_attention(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "        )\n",
    "        feed_forward_input = self.layernorm_1(inputs + multi_headed_self_attention_output)\n",
    "        feed_forward_output = self.feed_forward(feed_forward_input)\n",
    "        out = self.layernorm_2(feed_forward_input + feed_forward_output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(Layer):\n",
    "\n",
    "    def __init__(self, embedding_dim, dense_dim, num_heads, dropout_p, **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.masked_multi_headed_self_attention = attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.multi_headed_cross_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "        self.feed_forward = MultiLayerPerceptron(dense_dim, embedding_dim, dropout_p)\n",
    "        self.layernorm_3 = LayerNormalization()\n",
    "        self.dropout = Dropout(dropout_p)\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs):\n",
    "\n",
    "        causal_mask = generate_self_attention_mask(inputs)\n",
    "        masked_multi_headed_self_attention_output = self.masked_multi_headed_self_attention(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask    \n",
    "        )\n",
    "        masked_multi_headed_self_attention_output_normalized = self.layernorm_1(inputs + masked_multi_headed_self_attention_output)\n",
    "        masked_multi_headed_self_attention_output_normalized = self.dropout(masked_multi_headed_self_attention_output_normalized)\n",
    "\n",
    "        multi_headed_cross_attention_output = self.multi_headed_cross_attention(\n",
    "            query=masked_multi_headed_self_attention_output_normalized,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "        )\n",
    "\n",
    "        feed_forward_input = self.layernorm_2(masked_multi_headed_self_attention_output + masked_multi_headed_self_attention_output_normalized)\n",
    "        feed_forward_output = self.feed_forward(feed_forward_input)\n",
    "\n",
    "        out = self.layernorm_3(feed_forward_input + feed_forward_output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "dropout_p = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "x = EmbeddingBlock(english_vocab_size, embedding_dim, english_sequence_len)(encoder_inputs)\n",
    "encoder_block_outputs = EncoderBlock(embedding_dim, dense_dim, num_heads, dropout_p)(x)\n",
    "\n",
    "encoder = Model(encoder_inputs, encoder_block_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = Input(shape=(None, embedding_dim), name=\"encoded_seq_inputs\")\n",
    "\n",
    "x = EmbeddingBlock(spanish_vocab_size, embedding_dim, spanish_sequence_len)(decoder_inputs)\n",
    "x = DecoderBlock(embedding_dim, dense_dim, num_heads, dropout_p)(x, encoded_seq_inputs)\n",
    "x = Dropout(dropout_p)(x)\n",
    "decoder_block_outputs = Dense(spanish_vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "decoder = Model([decoder_inputs, encoded_seq_inputs], decoder_block_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_inputs = [encoder_inputs, decoder_inputs]\n",
    "transformer_outputs = decoder([decoder_inputs, encoder_block_outputs])\n",
    "\n",
    "transformer = Model(\n",
    "    transformer_inputs, transformer_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_block (EmbeddingBloc  (None, None, 256)   3412736     ['encoder_inputs[0][0]']         \n",
      " k)                                                                                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " encoder_block (EncoderBlock)   (None, None, 256)    3155456     ['embedding_block[0][0]']        \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 26213)  18711141    ['decoder_inputs[0][0]',         \n",
      "                                                                  'encoder_block[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,279,333\n",
      "Trainable params: 25,279,333\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay(epoch):\n",
    "        \n",
    "   initial_lrate = 1e-4\n",
    "   drop = 0.5\n",
    "   epochs_drop = 10.0\n",
    "   lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "   return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    optimizer=Adam(learning_rate=(1e-4)),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='model-epoch10+{epoch:02d}-loss{val_loss:.2f}.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1, \n",
    "    save_best_only=True,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_block/embedding/embeddings:0', 'embedding_block/embedding_1/embeddings:0', 'encoder_block/multi_head_attention/query/kernel:0', 'encoder_block/multi_head_attention/query/bias:0', 'encoder_block/multi_head_attention/key/kernel:0', 'encoder_block/multi_head_attention/key/bias:0', 'encoder_block/multi_head_attention/value/kernel:0', 'encoder_block/multi_head_attention/value/bias:0', 'encoder_block/multi_head_attention/attention_output/kernel:0', 'encoder_block/multi_head_attention/attention_output/bias:0', 'encoder_block/layer_normalization/gamma:0', 'encoder_block/layer_normalization/beta:0', 'dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'encoder_block/layer_normalization_1/gamma:0', 'encoder_block/layer_normalization_1/beta:0', 'decoder_block/multi_head_attention_2/query/kernel:0', 'decoder_block/multi_head_attention_2/query/bias:0', 'decoder_block/multi_head_attention_2/key/kernel:0', 'decoder_block/multi_head_attention_2/key/bias:0', 'decoder_block/multi_head_attention_2/value/kernel:0', 'decoder_block/multi_head_attention_2/value/bias:0', 'decoder_block/multi_head_attention_2/attention_output/kernel:0', 'decoder_block/multi_head_attention_2/attention_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_block/embedding/embeddings:0', 'embedding_block/embedding_1/embeddings:0', 'encoder_block/multi_head_attention/query/kernel:0', 'encoder_block/multi_head_attention/query/bias:0', 'encoder_block/multi_head_attention/key/kernel:0', 'encoder_block/multi_head_attention/key/bias:0', 'encoder_block/multi_head_attention/value/kernel:0', 'encoder_block/multi_head_attention/value/bias:0', 'encoder_block/multi_head_attention/attention_output/kernel:0', 'encoder_block/multi_head_attention/attention_output/bias:0', 'encoder_block/layer_normalization/gamma:0', 'encoder_block/layer_normalization/beta:0', 'dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'encoder_block/layer_normalization_1/gamma:0', 'encoder_block/layer_normalization_1/beta:0', 'decoder_block/multi_head_attention_2/query/kernel:0', 'decoder_block/multi_head_attention_2/query/bias:0', 'decoder_block/multi_head_attention_2/key/kernel:0', 'decoder_block/multi_head_attention_2/key/bias:0', 'decoder_block/multi_head_attention_2/value/kernel:0', 'decoder_block/multi_head_attention_2/value/bias:0', 'decoder_block/multi_head_attention_2/attention_output/kernel:0', 'decoder_block/multi_head_attention_2/attention_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_block/embedding/embeddings:0', 'embedding_block/embedding_1/embeddings:0', 'encoder_block/multi_head_attention/query/kernel:0', 'encoder_block/multi_head_attention/query/bias:0', 'encoder_block/multi_head_attention/key/kernel:0', 'encoder_block/multi_head_attention/key/bias:0', 'encoder_block/multi_head_attention/value/kernel:0', 'encoder_block/multi_head_attention/value/bias:0', 'encoder_block/multi_head_attention/attention_output/kernel:0', 'encoder_block/multi_head_attention/attention_output/bias:0', 'encoder_block/layer_normalization/gamma:0', 'encoder_block/layer_normalization/beta:0', 'dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'encoder_block/layer_normalization_1/gamma:0', 'encoder_block/layer_normalization_1/beta:0', 'decoder_block/multi_head_attention_2/query/kernel:0', 'decoder_block/multi_head_attention_2/query/bias:0', 'decoder_block/multi_head_attention_2/key/kernel:0', 'decoder_block/multi_head_attention_2/key/bias:0', 'decoder_block/multi_head_attention_2/value/kernel:0', 'decoder_block/multi_head_attention_2/value/bias:0', 'decoder_block/multi_head_attention_2/attention_output/kernel:0', 'decoder_block/multi_head_attention_2/attention_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_block/embedding/embeddings:0', 'embedding_block/embedding_1/embeddings:0', 'encoder_block/multi_head_attention/query/kernel:0', 'encoder_block/multi_head_attention/query/bias:0', 'encoder_block/multi_head_attention/key/kernel:0', 'encoder_block/multi_head_attention/key/bias:0', 'encoder_block/multi_head_attention/value/kernel:0', 'encoder_block/multi_head_attention/value/bias:0', 'encoder_block/multi_head_attention/attention_output/kernel:0', 'encoder_block/multi_head_attention/attention_output/bias:0', 'encoder_block/layer_normalization/gamma:0', 'encoder_block/layer_normalization/beta:0', 'dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'encoder_block/layer_normalization_1/gamma:0', 'encoder_block/layer_normalization_1/beta:0', 'decoder_block/multi_head_attention_2/query/kernel:0', 'decoder_block/multi_head_attention_2/query/bias:0', 'decoder_block/multi_head_attention_2/key/kernel:0', 'decoder_block/multi_head_attention_2/key/bias:0', 'decoder_block/multi_head_attention_2/value/kernel:0', 'decoder_block/multi_head_attention_2/value/bias:0', 'decoder_block/multi_head_attention_2/attention_output/kernel:0', 'decoder_block/multi_head_attention_2/attention_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "   1/3495 [..............................] - ETA: 1:45:22 - loss: 10.2072 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 12:30:25.199207: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x11140e930 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-02-14 12:30:25.199241: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Host, Default Version\n",
      "2023-02-14 12:30:25.205034: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-02-14 12:30:25.237681: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3495/3495 [==============================] - ETA: 0s - loss: 2.7850 - accuracy: 0.6389\n",
      "Epoch 1: val_loss improved from inf to 2.35997, saving model to model-epoch10+01-loss2.36.h5\n",
      "3495/3495 [==============================] - 490s 140ms/step - loss: 2.7850 - accuracy: 0.6389 - val_loss: 2.3600 - val_accuracy: 0.6619 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 2.2774 - accuracy: 0.6645\n",
      "Epoch 2: val_loss improved from 2.35997 to 2.20621, saving model to model-epoch10+02-loss2.21.h5\n",
      "3495/3495 [==============================] - 487s 139ms/step - loss: 2.2774 - accuracy: 0.6645 - val_loss: 2.2062 - val_accuracy: 0.6686 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 2.1606 - accuracy: 0.6725\n",
      "Epoch 3: val_loss improved from 2.20621 to 2.13213, saving model to model-epoch10+03-loss2.13.h5\n",
      "3495/3495 [==============================] - 490s 140ms/step - loss: 2.1606 - accuracy: 0.6725 - val_loss: 2.1321 - val_accuracy: 0.6776 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 2.0855 - accuracy: 0.6792\n",
      "Epoch 4: val_loss improved from 2.13213 to 2.08156, saving model to model-epoch10+04-loss2.08.h5\n",
      "3495/3495 [==============================] - 491s 141ms/step - loss: 2.0855 - accuracy: 0.6792 - val_loss: 2.0816 - val_accuracy: 0.6826 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 2.0326 - accuracy: 0.6830\n",
      "Epoch 5: val_loss improved from 2.08156 to 2.04347, saving model to model-epoch10+05-loss2.04.h5\n",
      "3495/3495 [==============================] - 491s 140ms/step - loss: 2.0326 - accuracy: 0.6830 - val_loss: 2.0435 - val_accuracy: 0.6857 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.9937 - accuracy: 0.6862\n",
      "Epoch 6: val_loss improved from 2.04347 to 2.01530, saving model to model-epoch10+06-loss2.02.h5\n",
      "3495/3495 [==============================] - 505s 145ms/step - loss: 1.9937 - accuracy: 0.6862 - val_loss: 2.0153 - val_accuracy: 0.6873 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.9615 - accuracy: 0.6885\n",
      "Epoch 7: val_loss improved from 2.01530 to 1.99502, saving model to model-epoch10+07-loss2.00.h5\n",
      "3495/3495 [==============================] - 488s 140ms/step - loss: 1.9615 - accuracy: 0.6885 - val_loss: 1.9950 - val_accuracy: 0.6890 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.9358 - accuracy: 0.6905\n",
      "Epoch 8: val_loss improved from 1.99502 to 1.97922, saving model to model-epoch10+08-loss1.98.h5\n",
      "3495/3495 [==============================] - 492s 141ms/step - loss: 1.9358 - accuracy: 0.6905 - val_loss: 1.9792 - val_accuracy: 0.6908 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.9122 - accuracy: 0.6922\n",
      "Epoch 9: val_loss improved from 1.97922 to 1.96137, saving model to model-epoch10+09-loss1.96.h5\n",
      "3495/3495 [==============================] - 496s 142ms/step - loss: 1.9122 - accuracy: 0.6922 - val_loss: 1.9614 - val_accuracy: 0.6915 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.8834 - accuracy: 0.6948\n",
      "Epoch 10: val_loss improved from 1.96137 to 1.95277, saving model to model-epoch10+10-loss1.95.h5\n",
      "3495/3495 [==============================] - 489s 140ms/step - loss: 1.8834 - accuracy: 0.6948 - val_loss: 1.9528 - val_accuracy: 0.6923 - lr: 5.0000e-05\n",
      "Epoch 11/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.8715 - accuracy: 0.6957\n",
      "Epoch 11: val_loss improved from 1.95277 to 1.94583, saving model to model-epoch10+11-loss1.95.h5\n",
      "3495/3495 [==============================] - 515s 147ms/step - loss: 1.8715 - accuracy: 0.6957 - val_loss: 1.9458 - val_accuracy: 0.6927 - lr: 5.0000e-05\n",
      "Epoch 12/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.8612 - accuracy: 0.6968\n",
      "Epoch 12: val_loss improved from 1.94583 to 1.94004, saving model to model-epoch10+12-loss1.94.h5\n",
      "3495/3495 [==============================] - 531s 152ms/step - loss: 1.8612 - accuracy: 0.6968 - val_loss: 1.9400 - val_accuracy: 0.6934 - lr: 5.0000e-05\n",
      "Epoch 13/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.8514 - accuracy: 0.6975\n",
      "Epoch 13: val_loss improved from 1.94004 to 1.93570, saving model to model-epoch10+13-loss1.94.h5\n",
      "3495/3495 [==============================] - 524s 150ms/step - loss: 1.8514 - accuracy: 0.6975 - val_loss: 1.9357 - val_accuracy: 0.6940 - lr: 5.0000e-05\n",
      "Epoch 14/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.8419 - accuracy: 0.6982\n",
      "Epoch 14: val_loss improved from 1.93570 to 1.93061, saving model to model-epoch10+14-loss1.93.h5\n",
      "3495/3495 [==============================] - 511s 146ms/step - loss: 1.8419 - accuracy: 0.6982 - val_loss: 1.9306 - val_accuracy: 0.6947 - lr: 5.0000e-05\n",
      "Epoch 15/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.8334 - accuracy: 0.6992\n",
      "Epoch 15: val_loss improved from 1.93061 to 1.92658, saving model to model-epoch10+15-loss1.93.h5\n",
      "3495/3495 [==============================] - 503s 144ms/step - loss: 1.8334 - accuracy: 0.6992 - val_loss: 1.9266 - val_accuracy: 0.6949 - lr: 5.0000e-05\n",
      "Epoch 16/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.8243 - accuracy: 0.6999\n",
      "Epoch 16: val_loss improved from 1.92658 to 1.92284, saving model to model-epoch10+16-loss1.92.h5\n",
      "3495/3495 [==============================] - 502s 144ms/step - loss: 1.8243 - accuracy: 0.6999 - val_loss: 1.9228 - val_accuracy: 0.6953 - lr: 5.0000e-05\n",
      "Epoch 17/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.8162 - accuracy: 0.7009\n",
      "Epoch 17: val_loss improved from 1.92284 to 1.91921, saving model to model-epoch10+17-loss1.92.h5\n",
      "3495/3495 [==============================] - 507s 145ms/step - loss: 1.8162 - accuracy: 0.7009 - val_loss: 1.9192 - val_accuracy: 0.6956 - lr: 5.0000e-05\n",
      "Epoch 18/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.8083 - accuracy: 0.7015\n",
      "Epoch 18: val_loss improved from 1.91921 to 1.91514, saving model to model-epoch10+18-loss1.92.h5\n",
      "3495/3495 [==============================] - 524s 150ms/step - loss: 1.8083 - accuracy: 0.7015 - val_loss: 1.9151 - val_accuracy: 0.6959 - lr: 5.0000e-05\n",
      "Epoch 19/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.8002 - accuracy: 0.7023\n",
      "Epoch 19: val_loss improved from 1.91514 to 1.91239, saving model to model-epoch10+19-loss1.91.h5\n",
      "3495/3495 [==============================] - 527s 151ms/step - loss: 1.8002 - accuracy: 0.7023 - val_loss: 1.9124 - val_accuracy: 0.6960 - lr: 5.0000e-05\n",
      "Epoch 20/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.7863 - accuracy: 0.7036\n",
      "Epoch 20: val_loss improved from 1.91239 to 1.91126, saving model to model-epoch10+20-loss1.91.h5\n",
      "3495/3495 [==============================] - 519s 148ms/step - loss: 1.7863 - accuracy: 0.7036 - val_loss: 1.9113 - val_accuracy: 0.6963 - lr: 2.5000e-05\n",
      "Epoch 21/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.7821 - accuracy: 0.7039\n",
      "Epoch 21: val_loss improved from 1.91126 to 1.90911, saving model to model-epoch10+21-loss1.91.h5\n",
      "3495/3495 [==============================] - 511s 146ms/step - loss: 1.7821 - accuracy: 0.7039 - val_loss: 1.9091 - val_accuracy: 0.6967 - lr: 2.5000e-05\n",
      "Epoch 22/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.7784 - accuracy: 0.7043\n",
      "Epoch 22: val_loss improved from 1.90911 to 1.90799, saving model to model-epoch10+22-loss1.91.h5\n",
      "3495/3495 [==============================] - 513s 147ms/step - loss: 1.7784 - accuracy: 0.7043 - val_loss: 1.9080 - val_accuracy: 0.6964 - lr: 2.5000e-05\n",
      "Epoch 23/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.7745 - accuracy: 0.7047\n",
      "Epoch 23: val_loss improved from 1.90799 to 1.90594, saving model to model-epoch10+23-loss1.91.h5\n",
      "3495/3495 [==============================] - 513s 147ms/step - loss: 1.7745 - accuracy: 0.7047 - val_loss: 1.9059 - val_accuracy: 0.6969 - lr: 2.5000e-05\n",
      "Epoch 24/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.7702 - accuracy: 0.7051\n",
      "Epoch 24: val_loss improved from 1.90594 to 1.90444, saving model to model-epoch10+24-loss1.90.h5\n",
      "3495/3495 [==============================] - 501s 143ms/step - loss: 1.7702 - accuracy: 0.7051 - val_loss: 1.9044 - val_accuracy: 0.6967 - lr: 2.5000e-05\n",
      "Epoch 25/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.7663 - accuracy: 0.7055\n",
      "Epoch 25: val_loss improved from 1.90444 to 1.90436, saving model to model-epoch10+25-loss1.90.h5\n",
      "3495/3495 [==============================] - 502s 144ms/step - loss: 1.7663 - accuracy: 0.7055 - val_loss: 1.9044 - val_accuracy: 0.6967 - lr: 2.5000e-05\n",
      "Epoch 26/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.7621 - accuracy: 0.7059\n",
      "Epoch 26: val_loss improved from 1.90436 to 1.90329, saving model to model-epoch10+26-loss1.90.h5\n",
      "3495/3495 [==============================] - 516s 148ms/step - loss: 1.7621 - accuracy: 0.7059 - val_loss: 1.9033 - val_accuracy: 0.6970 - lr: 2.5000e-05\n",
      "Epoch 27/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.7584 - accuracy: 0.7061\n",
      "Epoch 27: val_loss improved from 1.90329 to 1.90169, saving model to model-epoch10+27-loss1.90.h5\n",
      "3495/3495 [==============================] - 505s 145ms/step - loss: 1.7584 - accuracy: 0.7061 - val_loss: 1.9017 - val_accuracy: 0.6971 - lr: 2.5000e-05\n",
      "Epoch 28/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.7555 - accuracy: 0.7067\n",
      "Epoch 28: val_loss improved from 1.90169 to 1.89986, saving model to model-epoch10+28-loss1.90.h5\n",
      "3495/3495 [==============================] - 506s 145ms/step - loss: 1.7555 - accuracy: 0.7067 - val_loss: 1.8999 - val_accuracy: 0.6972 - lr: 2.5000e-05\n",
      "Epoch 29/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.7518 - accuracy: 0.7070\n",
      "Epoch 29: val_loss did not improve from 1.89986\n",
      "3495/3495 [==============================] - 508s 145ms/step - loss: 1.7518 - accuracy: 0.7070 - val_loss: 1.9001 - val_accuracy: 0.6974 - lr: 2.5000e-05\n",
      "Epoch 30/30\n",
      "3495/3495 [==============================] - ETA: 0s - loss: 1.7452 - accuracy: 0.7076\n",
      "Epoch 30: val_loss improved from 1.89986 to 1.89890, saving model to model-epoch10+30-loss1.90.h5\n",
      "3495/3495 [==============================] - 492s 141ms/step - loss: 1.7452 - accuracy: 0.7076 - val_loss: 1.8989 - val_accuracy: 0.6975 - lr: 1.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28e8e6920>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[checkpoint, lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model-epoch10+01-loss1.78.h5'\n",
    "\n",
    "transformer = load_model(model_path, custom_objects={\n",
    "    'EmbeddingBlock': EmbeddingBlock,\n",
    "    'MultiLayerPerceptron': MultiLayerPerceptron,\n",
    "    'EncoderBlock': EncoderBlock,\n",
    "    'DecoderBlock': DecoderBlock\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence, model,  max_output_len, spanish_token_lookup, word_sample_size):\n",
    "\n",
    "    translated_sentence = '<START>'\n",
    "    for i in range(max_output_len):\n",
    "\n",
    "        tokenized_target_sentence = pad_and_tokenize(\n",
    "            translated_sentence,\n",
    "            spanish_tokenizer,\n",
    "            spanish_sequence_len\n",
    "        )[:-1]\n",
    "\n",
    "        encoder_inputs = input_sentence['encoder_inputs']\n",
    "        decoder_inputs = np.expand_dims(tokenized_target_sentence, axis=0)\n",
    "\n",
    "        predictions = model([encoder_inputs, decoder_inputs])[0]\n",
    "        \n",
    "        top_n_pred_tokens = np.argpartition(predictions[i, :], -word_sample_size)[-word_sample_size:]\n",
    "        pred_token = np.random.choice(top_n_pred_tokens, size=1)[0]\n",
    "\n",
    "        if pred_token:\n",
    "            sampled_word = spanish_token_lookup[pred_token]\n",
    "            translated_sentence += ' ' + sampled_word\n",
    "\n",
    "        if pred_token == '<END>':\n",
    "            break\n",
    "    else:\n",
    "        translated_sentence += ' <END>'\n",
    "\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X, y in test_ds.take(3):\n",
    "\n",
    "#     actual_eng = get_sentence(X['encoder_inputs'][0].numpy(), english_token_lookup)\n",
    "#     translated = translate(X, transformer, spanish_sequence_len-1, spanish_token_lookup, 2)\n",
    "\n",
    "#     print('ACTUAL:', actual_eng)\n",
    "#     print('TRANSLATED:', translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3bfd8a20af1d5d4753b5a38df58ddc27d816f3a7f519b02fcfafca237454594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
